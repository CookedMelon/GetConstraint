@keras_export("keras.backend.clear_session")
def clear_session():
    """Resets all state generated by Keras.
    Keras manages a global state, which it uses to implement the Functional
    model-building API and to uniquify autogenerated layer names.
    If you are creating many models in a loop, this global state will consume
    an increasing amount of memory over time, and you may want to clear it.
    Calling `clear_session()` releases the global state: this helps avoid
    clutter from old models and layers, especially when memory is limited.
    Example 1: calling `clear_session()` when creating models in a loop
    ```python
    for _ in range(100):
      # Without `clear_session()`, each iteration of this loop will
      # slightly increase the size of the global state managed by Keras
      model = tf.keras.Sequential([
          tf.keras.layers.Dense(10) for _ in range(10)])
    for _ in range(100):
      # With `clear_session()` called at the beginning,
      # Keras starts with a blank state at each iteration
      # and memory consumption is constant over time.
      tf.keras.backend.clear_session()
      model = tf.keras.Sequential([
          tf.keras.layers.Dense(10) for _ in range(10)])
    ```
    Example 2: resetting the layer name generation counter
    >>> import tensorflow as tf
    >>> layers = [tf.keras.layers.Dense(10) for _ in range(10)]
    >>> new_layer = tf.keras.layers.Dense(10)
    >>> print(new_layer.name)
    dense_10
    >>> tf.keras.backend.set_learning_phase(1)
    >>> print(tf.keras.backend.learning_phase())
    1
    >>> tf.keras.backend.clear_session()
    >>> new_layer = tf.keras.layers.Dense(10)
    >>> print(new_layer.name)
    dense
    """
    global _SESSION
    global _GRAPH_LEARNING_PHASES
    global _GRAPH_VARIABLES
    global _GRAPH_TF_OPTIMIZERS
    global _GRAPH
    _GRAPH.graph = None
    tf.compat.v1.reset_default_graph()
    reset_uids()
    if _SESSION.session is not None:
        _SESSION.session.close()
        _SESSION.session = None
    graph = get_graph()
    with graph.as_default():
        _DUMMY_EAGER_GRAPH.learning_phase_is_set = False
        _GRAPH_LEARNING_PHASES = {}
        # Create the learning phase placeholder in graph using the default
        # factory
        phase = _default_learning_phase()
        _internal_set_learning_phase(graph, phase)
        _GRAPH_VARIABLES.pop(graph, None)
        _GRAPH_TF_OPTIMIZERS.pop(graph, None)
    if tf.executing_eagerly():
        # Clear pending nodes in eager executors, kernel caches and
        # step_containers.
        context.context().clear_kernel_cache()
